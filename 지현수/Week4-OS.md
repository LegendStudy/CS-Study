# CS_Study-OS

# 개요

---

어짜피 다들 같은 책을 보고 공부했으니까 / 공부한 내용을 되짚어 보면서

왜? 라는 생각을 가지고 공부를 했습니다.

# 3.1 운영체제와 컴퓨터

---

## 운영체제는 무엇일까요?

운영체제(이하 OS)는 크게 커널(kernel) 모드와 사용자(user) 모드로 나뉘어 진다.

- 커널 모드(kernel mode)
    - 모든 시스템 메모리에 접근 가능
    - 모든 CPU 명령 실행 가능
- 사용자 모드(user mode): 사용자 애플리케이션 실행,
    - 사용자 에플리케이션 실행
    - 하드웨어 직접 접근 불가
    - System call 호출 시 일시적으로 커널 모드로 전환

위의 사용자 모드의 특징에 있는 것처럼 **system call은 커널 영역의 기능을 사용자 모드가 사용하게 하는 것으로 즉, 프로세스가 하드웨어에 직접 접근해서 필요한 기능을 사용할 수 있게** 해준다.

운영체제는 하드웨어 자원을 관리하고, 응용 프로그램과 하드웨어 사이를 중재하는 인터페이스를 의미합니다.

---

## 운영체제의 역할은 크게 네 가지가 있습니다!

1. CPU스케줄링과프로세스관리: CPU소유권을어떤프로세스에할당할지, 프로세스의생성과삭제, 자원할당및반환을관리합니다.
2. 메모리관리: 한정된메모리를어떤프로세스에얼마큼할당해야하는지관리합니다.
3. 디스크파일관리: 디스크파일을어떠한방법으로보관할지관리합니다.
4. I/0디바이스관리:I/0디바이스들인마우스, 키보드와컴퓨터간에데이터를주고받는것을관리합니다.

---

## 운영체제의 구조중에서 커널은 무엇일까요?

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled.png)

기본적으로 프로그램이 실행되기 위해서는 주기억장치에 적재된 상태여야합니다.
운영체제 역시 프로그램이기 때문에 메인메모리에 적재가 되어야 하나, 운영체제의 크기가 너무 커서 전체를 다 메인 메모리에 올리게 된다면 비효율적입니다. (메모리 공간의 낭비)

따라서 커널(Kernel)이라고 하는 항상 필요한 운영체제의 핵심 부분만을 메인 메모리에 적재하여 운영체제를 사용하게끔 합니다.

즉, 커널은 메모리에 상주하는 운영체제의 핵심 부분이라고 할 수 있습니다.

---

## System Call은 어떻게 발생할까요?

먼저 유저모드에서 사용자가 특정 응용프로그램 실행이나 변경이 있을때 운영체제에서는 인터럽트를 발생시킵니다. 이는 운영체제가 인터럽트 서비스 루틴을 수행하기 때문에 모든 이벤트에 대해서 인터럽트를 발생시킵니다!

인터럽트가 발생 했을때 해당하는 행위에 대한 시스템 콜이 발생되고 modebit를 1에서 0으로 변경시켜 커널모드로 진입하게 됩니다.

커널모드에서는 요구사항에 대한 로직을 수행하게 되고 완전히 마친 후 modebit를 다시 1로 바꾸어 유저모드에서 나머지 로직을 수행하게 됩니다.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%201.png)

---

## 그러면 System Call은 왜 중요할까요?

**User가 에플리케이션으로 OS의 치명적인 데이터를 수정/삭제하는 권한을 막기 위해서**다.

OS는 프로세스의 실행, 종료나 I/O 작업 등의 사용자가 함부로 사용하면 문제가 될 만한 명령들을 **privileged Instruction**으로 분류하여 막아놓았다.

따라서 사용자들이 직접적인 하드웨어 요청이나 기타 시스템요청을 하려면, OS가 제공하는 System call을 통해 호출하도록 제공한다.

유저 애플리케이션이 system call을 호출하여 사용하면 해당 애플리케이션은 커널모드로 잠시 전환되는 작업을 거치게 되며, **커널 모드를 통한 이러한 작업은 반드시 시스템 콜을 통해 수행하도록 설계되어 있다.**

> Privileged Instruction
> 
> - 프로세스의 실행, 종료나 I/O 작업 등의 사용자가 함부로 사용하면 문제가 될 만한 명령들을 말한다.
> - OS의 kernel mode와 user mode 중 kernel mode에서만 사용이 가능하며, 이와 같은 기능을 사용하기 위해서 user mode에서 kernel mode로 trap하는 system call을 사용한다.

---

## 인터럽트는 무엇인가요?

> 인터럽트(Interrupt)는 시스템의 내부 또는 외부에서 발생하는 예기치 못한 사건에 의해, 프로세서(흔히 CPU)가 실행 중인 프로그램의 작업을 중단시키고 다른 프로그램을 수행하도록 하는 명령어이다.
> 

여기서 말하는 예기치 못한 사건은 입출력(Input/Output or I/O)이나 에러의 발생, 타이머의 시간 만료 등을 의미한다.

인터럽트의 종류는 두 가지인데, 하드웨어 인터럽트와 소프트웨어 인터럽트가 있다. 말 그대로 '하드웨어에 의한 인터럽트냐, 소프트웨어에 의한 인터럽트냐'이다.

1. 하드웨어 인터럽트(hardware interrupts): 컴퓨터의 외부 하드웨어 기기인 키보드나 마우스, 네트워크 카드에 의한 인터럽트를 가리킨다.

2. 소프트웨어 인터럽트(software interrupts): CPU에 의해 실행 되고 있는 프로그램에 의한 인터럽트를 가리킨다.

---

## 왜 운영체제는 인터럽트 서비스 루틴을 사용할까?

1. 인터럽트는 현대의 운영체제가 하드웨어와의 상호작용을 향상시켜, CPU가 멀티 프로그래밍을 가능하게 하는 핵심적인 역할을 한다.

2. 인터럽트는 갑작스러운 컴퓨터의 전류 공급 중단이나, 사용자의 입력, 입출력 작업의 완료, 운영체제에 의한 긴급 요청 등과 같은 긴급한 상황에 대처하기 위해서도 필요하다.

만약 인터럽트가 없다면 CPU는 장치의 상태를 수시로 점검해야 하는 상황이 발생하는데, 그동안 다른 작업을 수행할 수 없어서 CPU의 사용성(utilization)을 낭비시킨다.

<aside>
🔑 멀티프로세서의 기능을 갖추기 위해서 운영체제는 인터럽트를 사용한다!

</aside>

---

# 3.2 메모리

---

## 메모리는 어떤 계층이 있을까요?

1. 레지스터: CPU가 연산할 때 주로 사용하는 메모리입니다. 

2. 캐시: L1, L2, L3 캐시가 있고 데이터를 미리 복사해 놓은 임시 저장소이자 빠른장치(주기억장치) 느린장치(보조기억장치)의 속도 차이에 따른 병목 현상을 줄이기 위한 메모리입니다.

3. 주기억장치: 우리가 사용하는 RAM이 주기억장치입니다
4. 보조기억장치: HDD, SSD와 같은 데이터 저장기기가 보조기억장치입니다.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%202.png)

## 캐시 매핑 분류는 어떤 것이 있을까요?

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%203.png)

캐쉬 매핑이란 캐시가 히트되기 위해 매핑하는 방법을 말합니다 현대에서는 집합 연관 매핑을 주로 사용하고있고 직접 매핑과 연관 매핑도 Intell, AMD사 CPU에서 알고리즘을 통해 적절할 때 사용하고 있습니다.

- **L1 캐시**: 가장 빠르고 작은 캐시로, 일반적으로 2-way 또는 4-way 집합 연관 매핑을 사용합니다.
- **L2 캐시**: L1 캐시보다 크고 약간 느리며, 일반적으로 4-way에서 16-way 집합 연관 매핑을 사용합니다. 이는 더 많은 데이터를 캐시에 저장하면서도 충돌을 최소화합니다.
- **L3 캐시**: 여러 코어가 공유하는 큰 캐시로, 보통 16-way 이상 집합 연관 매핑을 사용합니다.

---

## 예시를 한번 생각해봅시다! 
우리가 사용하는 Intellij의 응용프로그램이 CPU에서 인터럽트 됐을 때 캐시에 존재할 때와 존재하지 않을 때 어떻게 작동할까요?

---

먼저 CPU는 PC register, Status register를 이용하여 메모리에 탑재되어 있는 intellij의 데이터를 가져오려고 할 것입니다! 이때 캐시에 해당하는 데이터를 가지고 있다면,

Cache Hit → 빠르게 데이터를 가져올 수 있게 됩니다.

만약 캐시에 해당하는 데이터가 없다면,

Cache Miss →  주기억장치(RAM)에서 해당하는 메모리 주소를 탐색하여 Bus를 통해 데이터를 CPU로 가져오게 됩니다. 그 후 데이터를 캐시에 저장하고 만약 캐시가 꽉 차있다면 캐시교체 알고리즘을 통해 캐시에 오랫동안 사용되지 않는 데이터를 제거하고 데이터를 캐시에 저장합니다.

---

## 메모리에서 페이지 폴드는 무엇인가요?

페이지 폴드란 프로세스 주소 공간에는 존재하지만 지금 이 컴퓨터의 RAM에는 없는 데이터에 접근 했을 경우에 발생합니다. 

페이지 폴트와 그로 인한 스와핑은 다음 과정으로 이루어집니다.

1. 어떤 명령어가 유효한 가상주소에 접근했으나 해당 페이지가 만약 없다면 트랩이 발생되어 운영체제에 알리게 됩니다. 

2. 운영체제는 실제 디스크로부터 사용하지 않은 프레임을 찾습니다.

3. 해당 프레임을 실제 메모리에 가져와서 페이지 교체 알고리즘을 기반으로 특정페이지와 교체 합니다 (이때 스와핑이 일어납니다).

4. 페이지 테이블을 갱신 시킨 후 해당 명령어를 다시 시작합니다.

---

## 현대 운영체제가 사용하는 방법으로 페이징 기법은 무엇인가요?

**연속 할당의 문제점 - 외부 단편화(External Fragmentation)**

연속 할당이란 한 프로세스의 모든 논리적 주소에 동일한 base register를 더해주는 것이다. 이는 MMU에 의해 이뤄지며 주소는 Execution time에 바인딩 된다는 특징이 있다. 모든 프로세스는 논리적 주소가 0번지부터 시작해서 차례대로 증가하기 때문에, 물리적 주소도 시작 주소만 다르지 연속적으로 배치되어 있다.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%204.png)

<aside>
💡 홀(hole): 할당할 수 있는 비어 있는 메모리 공간.
파티션(Partition): 프로세스 메모리 공간
페이지(page): 가상 메모리를 사용하는 최소 크기 단위.

</aside>

운영체제는 Hole을 제외한 파티션의 개수 만큼 동시 실행 가능하다. 만약에 새로운 프로그램을 실행하려고 하는데, **요구하는 프로세스의 용량이 Hole의 크기보다 크면 메모리에 올라갈 수 없으므로 실행되지 못한다**. 이를 잘 기억해놓자. 이제 아래와 같은 상황에서 '프로세스 F' 를 실행시킬 수 있는지 판단해보자.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%205.png)

프로세스 F가 들어갈 **연속된 자리**가 없으므로 당연히 프로세스 F는 실행 불가능하다. 그런데 따지고 보면 프로세스 F가 들어갈 빈 공간은 충분하다. 위에서 흰색 칸이 Hole에 해당하는데, Hole의 합은 60MB로 프로세스 F의 크기보다 크기 때문이다. 이렇게 메모리에 공간이 있음에도 불구하고 연속된 Hole의 크기가 프로세스의 크기보다 작다는 이유로 할당되지 못하는 문제를 **외부 단편화**라고 한다.

### **외부 단편화 해결 방법 - 불연속 할당**

외부 단편화의 발생 원인은 '연속 할당'이었다. 그러면 주소를 불연속적으로 할당하면? 당연히 외부 단편화는 발생하지 않는다. 이렇게 주소를 불연속적으로 할당하는 메모리 관리 구조를 **Paging**이라고 한다.

### **Paging**

**외부 단편화의 해결 방법으로, 주소를 불연속적으로 할당하는 메모리 관리 구조**를 말한다.

<aside>
💡 Page(페이지): 가상 메모리를 일정한 크기로 나눈 블록
Frame(프레임): 물리 메모리를 일정한 크기로 나눈 블록

Page Size = Frame Size

</aside>

페이지와 프레임의 크기를 일정한 크기로 나눈 이유는 메모리의 효율적 관리를 위해서다. **페이지와 프레임의 크기는 같은데**, 이렇게 함으로써 페이지에 대응되는 프레임을 좀 더 쉽게 찾을 수 있다. 요점은 페이지의 크기가 정해지면 이 크기가 메모리 단위가 된다는 것이다. 참고로 페이지의 크기는 CPU에서 정의된다.

이제 모든 프로세스는 페이지 크기만큼 조각화되고, 물리적 메모리도 페이지 크기만큼 조각화된다. 그리고 프로세스의 각 페이지는 물리적 메모리의 조각인 프레임에 불연속적으로 할당된다. 이때, **페이지와 프레임의 대응 관계도는 '페이지 테이블'에 저장되어 있다**.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%206.png)

그래서 결론! 페이징은 가상메모리에서 연속 할당의 문제점인 외부 단편화를 해결하고 메모리를 효율적으로 관리하기 위한 것이다.

---

## 페이지 교체 알고리즘에는 어떤 것이 있을까?

### **FIFO 페이지 교체 알고리즘(First In First Out)**

시간상으로 물리 메모리에 가장 먼저 들어온 페이지를 대상 페이지로 선정하여 스왑 아웃시킨다. 페이지 부재일 때는 F, 원하는 페이지가 메모리에 있는 경우는 S로 표시되어 있다.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%207.png)

큐로 구현되어 있어, 새로운 페이지는 항상 맨 아래에 삽입되는데, 이때, 메모리가 꽉 차면, 맨 위의 페이지가 스왑 아웃되고 나머지 페이지들이 위쪽으로 이동하여, 새로운 페이지가 맨 아래로 들어온다.

이 그림에서는 총 열 번의 페이지 요구에 대해 세 번만 성공하고, 페이지 부재는 일곱 번 발생했다. 페이지 교체 알고리즘에서는 앞으로 사용하지 않을 페이지를 스왑 아웃시키는 것이 중요한데, FIFO 알고리즘은 그렇지 않다. 따라서, 성능이 떨어진다.

### **LRU 페이지 교체 알고리즘(Least Recently Used)**

메모리에 올라온 후 가장 오랫동안 사용되지 않은 페이지를 스왑 아웃시키는 알고리즘이다. 가장 오랫동안 사용되지 않은 페이지를 판단하는 방법에 따라 구현 방식은 여러 가지이다.

**페이지 접근 시간에 기반한 구현**

가장 간단하게 판단하는 방법은 페이지에 접근한 시각을 기록하여 판단하는 것이다.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%208.png)

메모리를 추가적으로 사용해서 각 페이지에 접근한 시각을 기록한다. 4번 시점에서 가장 페이지 접근 시간이 적은 페이지는 A이다. 따라서, 페이지 A를 대상 페이지로 선정하여 스왑 아웃시키고 그 자리에 페이지 D를 스왑 인시킨다.

**카운터에 기반한 구현**

가장 오랫동안 사용되지 않은 페이지를 판단하는 다른 방법으로는 카운터를 사용하여 구현할 수도 있다. 즉, 메모리를 추가적으로 사용해서 페이지에 접근한 시각이 아닌, 몇 번째로 접근했는지를 기록하는 것이다.

사실상, 페이지 접근 시간을 기록하는 방식이랑 다를 바가 없다. 이 두 가지 방식 모두 메모리 공간이 추가로 더 들어간다는 단점이 존재한다.

**참조 비트 시프트 방식**

각 페이지에 일정 크기의 참조 비트를 만들어 사용하는 것이다. 참조 비트의 초깃값은 0이며 페이지에 접근할 때마다 1로 바뀐다. 또한, 참조 비트는 주기적으로 한 칸씩 오른쪽 시프트(>>) 된다.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%209.png)

이와 같은 방식으로 참조 비트를 갱신하다가 대상 페이지를 선정할 필요가 있으면 참조 비트 중 가장 작은 값을 대상 페이지로 선정한다. 위 그림에서 (c) 시점에서 대상 페이지를 선정하려고 하면 당연히 C < A < B이므로 C가 대상 페이지가 된다.

<aside>
💡 정리: LRU 페이지 교체 알고리즘은 메모리가 추가로 필요하여 낭비되는 메모리 공간이 많아진다는 단점을 가지고 있다.

</aside>

### **LFU 페이지 교체 알고리즘(Least Frequently Used)**

페이지가 몇 번 사용되었는지를 기준으로 대상 페이지를 선정한다. 즉, 현재 프레임에 있는 페이지마다 그동안 사용된 횟수를 세어 횟수가 가장 적은 페이지를 스왑 아웃시킨다.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%2010.png)

위 그림에서 프레임 안에서 알파벳 아래의 숫자는 사용 빈도를 나타낸다. 이때, 사용 빈도가 같은 경우에는 FIFO, LRU, 무작위와 같은 추가 기준 알고리즘을 사용하여 대상 페이지를 선정하지만, 이 교재에서는 간단하게 맨 위의 페이지를 대상 페이지로 선정한다고 가정했다.

따라서, 4번 시점에서는 페이지 A가 대상 페이지로 선정되고, 6번 시점에서는 페이지 A와 C의 사용 빈도가 가장 적고 같아 페이지 A가 대상 페이지로 선정되었다.

알고리즘의 성능은 LRU 알고리즘과 비슷하다고 알려져 있다. 그러나, 페이지 접근 횟수(빈도)를 표시하는 데 LRU 알고리즘과 마찬가지로 메모리의 추가 공간이 필요하므로 낭비되는 메모리 공간이 많다는 단점이 존재한다.

### **NUR 페이지 교체 알고리즘(Not Used Recently)**

LRU 페이지 교체 알고리즘과 성능이 거의 비슷하면서도 불필요한 메모리 공간 낭비 문제를 해결한 페이지 교체 알고리즘이다. NUR 페이지 교체 알고리즘에서는 추가적인 메모리 공간을 사용하지 않는다. 참조 비트, 변경 비트를 이용해서 대상 페이지를 선정한다.

접근 비트는 페이지에 접근, 즉 읽기(read)나 실행(execute) 연산이 되면 1이 되고, 변경 비트는 페이지가 변경, 즉 쓰기(write)나 추가(append) 연산이 되면 1이 된다. 따라서, 초기 상태 (0, 0)에서부터 시작하여 '접근'이 발생하면 (1, 0), '변경'이 발생하면 (0, 1)이 된다. 또한, 두 가지 연산이 다 발생하면 (1, 1)이 된다.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%2011.png)

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%2012.png)

위 그림은 NUR 알고리즘에서 대상 페이지를 선정하는 순서를 보여준다. 순서를 보면 알 수 있듯이 우선 고려 대상은 접근(참조) 비트이다. 접근 비트가 0인 페이지를 먼저 찾고, 없으면 변경 비트가 0인 페이지를 찾는다.

만약, 같은 비트의 페이지가 여러 개라면 무작위, FIFO 등 다양한 방식의 추가 기준을 사용하여 대상 페이지를 선정한다. 또한, 흔한 상황은 아니지만 만약 모든 페이지가 (1, 1)이라면 모든 페이지 비트를 (0, 0)으로 초기화한다. 아래 그림은 NUR 페이지 교체 알고리즘 동작 과정이다. 같은 비트의 페이지가 여러 개일 때 맨 위 페이지를 대상 페이지로 선정한다는 가정을 참고하고 봐주길 바란다.

![Untitled](CS_Study-OS%202edee0b121e848db8c57c5346d21f990/Untitled%2013.png)

알고리즘의 성능은 LRU, LFU 알고리즘과 비슷하지만 낭비되는 메모리 공간이 없다는 장점이 있다. 이 교재에서는 NUR이 이러한 장점 때문에 널리 사용되고 있다고 했지만, 실제로는 LRU 알고리즘을 많이 사용하고 있다고 한다.

NUR 알고리즘을 채택하지 않는 이유는 각 페이지의 참조를 유지 및 업데이트하고 비트를 수정하기 위해 추가적인 하드웨어 지원이나 소프트웨어 오버헤드가 필요하기 때문이라고 한다. 반면에, LRU는 LFU, NUR보다 구현이 비교적 쉽고 오버헤드가 적기 때문에 많이 사용되는 알고리즘이라고 한다.